{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88f04e4e-70c9-43c4-b067-2f9a018866e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import cfgrib\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import fortranformat as ff\n",
    "from glob import glob\n",
    "# from astropy.io import ascii\n",
    "# from astropy.table import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "168a699c-c219-42ec-b55d-56e5f3f3ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "infolder_list = [r'G:\\Modeling\\223130\\HRRR\\02.03.2023.noaa.sg',\n",
    "                 r'G:\\Modeling\\223130\\HRRR\\02.04.2023.noaa.sg',\n",
    "                 r'G:\\Modeling\\223130\\HRRR\\02.05.2023.noaa.sg',\n",
    "                 r'G:\\Modeling\\223130\\HRRR\\02.06.2023.noaa.sg',\n",
    "                 r'G:\\Modeling\\223130\\HRRR\\02.07.2023.noaa.sg',\n",
    "                 r'G:\\Modeling\\223130\\HRRR\\02.08.2023.noaa.sg']\n",
    "\n",
    "out_asciifolder = r'P:\\CMaderia\\test'  #'G:\\Modeling\\223130\\HRRR\\ascii_output'\n",
    "out_logfolder = r'P:\\CMaderia\\test\\log'  #r'G:\\Modeling\\223130\\HRRR\\ascii_output\\log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "273a6ada-368b-481f-9a60-a82c5df75140",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_vars = ['MYR', 'MMO', 'MDAY', 'MHR', 'IX', 'JX', 'PRES', 'RAIN', 'SC', 'RADSW', 'RADLW', 'T2', 'Q2', 'WD10', 'WS10', 'SST']\n",
    "vertical_vars = ['PRES', 'Z', 'TEMPK', 'WD', 'WS', 'W', 'RH', 'VAPMR', 'CLDMR', 'RAINMR', 'ICEMR', 'SNOWMR', 'GRPMR'] \n",
    "# 'ICEMR' not available in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a6cce3-8f4f-4783-9656-09d3da709b57",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1) Formulas\n",
    "2) Unit conversions\n",
    "3) Rename columns\n",
    "4) Formatting (float/int decimal places)\n",
    "5) Combine data frames\n",
    "6) Export as CSV\n",
    "7) Export as ASCII\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4399ee-d032-4478-a3fc-d7be5cdeee9f",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c9a52f-04fc-438f-ac61-5cc02e4362f5",
   "metadata": {},
   "source": [
    "#### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3a55763-2ffc-4a38-ae90-1349b3cac572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new pandas dataframe from an xarray input\n",
    "def create_df(dataset, fieldnames:list):\n",
    "    dataset = dataset.get(fieldnames)\n",
    "    return dataset.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cae2661-c8ed-4306-80c1-f897db018a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate wind speed (dataframe)\n",
    "def calc_windspeed(dataframe, ufield:str, vfield:str):\n",
    "    magnitude = np.sqrt(np.square(dataframe[ufield]) + np.square(dataframe[vfield]))\n",
    "    \n",
    "    return magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a337aad-2f56-4482-8d93-03dd7566b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate wind direction (dataframe)\n",
    "def calc_winddir(dataframe, longfield:str, ufield:str, vfield:str):\n",
    "    angle2 = 0.622515*((dataframe[longfield]-360)+97.5)*0.017453\n",
    "    \n",
    "    # calculate U and V components\n",
    "    Un = np.cos(angle2) * dataframe[ufield] + np.sin(angle2) * dataframe[vfield]\n",
    "    Vn = (-1) * np.sin(angle2) * dataframe[ufield] + np.cos(angle2) * dataframe[vfield]\n",
    "    # wind direction (degrees)\n",
    "    angle = (270 - np.arctan2(Vn, Un) * 180 / math.pi)\n",
    "    \n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5a81d42-d275-45c2-b772-4ad248c98641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse out day, month, year\n",
    "def parse_date(gribname, dataframe, datefield:str):\n",
    "    dataframe['MYR'] = dataframe[datefield].astype(str).str.split('-').str[0]\n",
    "    dataframe['MMO'] = dataframe[datefield].astype(str).str.split('-').str[1]\n",
    "    dataframe['MDAY'] = dataframe[datefield].astype(str).str.split('-').str[2].str.split(' ').str[0]\n",
    "    dataframe['MHR'] = gribname.split('.')[1][1:3]  # take this one from the filename b/c it's not always in the date field\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f3ee386-4cb5-4f8d-b143-8ee34e6c8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change field type\n",
    "def change_field_type(dataframe, field_names:list, new_fieldtype:str):\n",
    "    #int, float, bool, str/object\n",
    "    for f in field_names:\n",
    "        if new_fieldtype in ['int', 'int32', 'int64', 'float', 'float32', 'float64']:\n",
    "            # convert to float first, then round up\n",
    "            dataframe[f] = dataframe[f].astype(float)\n",
    "            dataframe[f] = dataframe[f].round()\n",
    "            # then convert to new field type\n",
    "            dataframe[f] = dataframe[f].astype(new_fieldtype)\n",
    "        else:\n",
    "            # convert to new field type\n",
    "            dataframe[f] = dataframe[f].astype(new_fieldtype)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17666934-5d5f-4121-822b-62acaf99e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given two data frames, removes duplicate columns between them\n",
    "# src: https://www.geeksforgeeks.org/prevent-duplicated-columns-when-joining-two-pandas-dataframes/\n",
    "def remove_dup_cols(df_removedups, df_main, cols_to_keep:list):\n",
    "    # df_main = retains all columns\n",
    "    # df_removedups = retains only unique columns (between the two data frames)\n",
    "    # cols_to_keep = any duplicate columns that should be kept (i.e., for merging)\n",
    "    # returns second data frames with duplicate columns removed\n",
    "        different_cols = df_removedups.columns.difference(df_main.columns)\n",
    "        df_removedups = df_removedups[list(different_cols) + cols_to_keep]\n",
    "        return df_removedups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b922aa5-06ed-4d4b-8aac-ec7942d5e64d",
   "metadata": {},
   "source": [
    "#### vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33aa4b46-5982-4361-b1f6-8f88e120c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do HRRR surface conversions\n",
    "def calc_RH(dataframe, specific_humidity:str, pressure:str, temp:str):\n",
    "    # Pressure in Pa, height in gpm, temp in K\n",
    "    rh = (dataframe[specific_humidity]) / (0.622 * 6.112/dataframe[pressure] * \\\n",
    "                                                      np.exp(17.67 * (dataframe[temp]-273.15)/(dataframe[temp]-29.65)))\n",
    "    return rh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8ca120-ebb6-4926-9e40-47b453564757",
   "metadata": {},
   "source": [
    "#### surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e5099cd-05d1-4679-a465-70b5703e1d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do HRRR surface conversions\n",
    "def calc_slpressure(dataframe, pressure:str, height:str, temp:str):\n",
    "    # Pressure in Pa, height in gpm, temp in K\n",
    "    pres = (dataframe[pressure] * \\\n",
    "                         np.power((1 - 0.0065*dataframe[height]/(dataframe[temp]+0.0065*dataframe[height])), -5.257)) / 100\n",
    "    return pres\n",
    "\n",
    "def calc_rainfall(dataframe, rain:str):  \n",
    "    # RAIN  = prate (kg/m2/s to cm)\n",
    "    rainfall = dataframe[rain] * 60 * 60 * 0.1\n",
    "    return rainfall\n",
    "\n",
    "def calc_snowcover(dataframe, snow:str):\n",
    "    # SC - 50+ % = 1\n",
    "    dataframe.loc[dataframe[snow]<50.0, snow] = 0\n",
    "    dataframe.loc[dataframe[snow]>=50.0, snow] = 1\n",
    "    sc = dataframe[snow]\n",
    "    return sc\n",
    "\n",
    "# no conversion needed for these\n",
    "# RADSW = dswrf\n",
    "# RADLW = ulwrf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12fc8ab-ef6d-43b7-8d76-ef361ee5e6df",
   "metadata": {},
   "source": [
    "#### run functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c72ac8e-6b2d-40b2-921f-c467866c91cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERTICAL\n",
    "# in = path of grib file, out = Fortran-formatted dataframe\n",
    "def process_hybrid(grib_in, log_out):\n",
    "    ds = xr.open_dataset(grib_in, engine='cfgrib', \\\n",
    "                         backend_kwargs={'filter_by_keys': {'typeOfLevel': 'hybrid'}})\n",
    "    \n",
    "    df_hybrid = create_df(ds, ['pres', 'gh', 't', 'u', 'v', 'w', 'q', 'clwmr', 'rwmr', 'snmr', 'grle'])\n",
    "    \n",
    "    df_hybrid = df_hybrid.reset_index()  # so x, y show up\n",
    "\n",
    "    # calculate relative humidity first\n",
    "    df_hybrid['RH'] = calc_RH(df_hybrid, 'q', 'pres', 't')\n",
    "\n",
    "    # add 10 to x (i) and y (j)\n",
    "    df_hybrid['x'] = df_hybrid['x'] + 10\n",
    "    df_hybrid['y'] = df_hybrid['y'] + 10\n",
    "\n",
    "    # do HRRR hybrid level conversions\n",
    "    df_hybrid['PRES'] = df_hybrid['pres'] / 100  # Pa to hPa (mb)\n",
    "    #df_hybrid.drop('pres', axis=1, inplace=True)\n",
    "    df_hybrid['WD'] = calc_winddir(df_hybrid, 'longitude', 'u', 'v')\n",
    "    df_hybrid['WS'] = calc_windspeed(df_hybrid, 'u', 'v')\n",
    "    df_hybrid['q'] = df_hybrid['q'] * 1000\n",
    "    df_hybrid['CLDMR'] = df_hybrid['clwmr'] * 1000\n",
    "    df_hybrid['RAINMR'] = df_hybrid['rwmr'] * 1000\n",
    "    df_hybrid['ICEMR'] = 0.0\n",
    "    df_hybrid['SNOWMR'] = df_hybrid['snmr'] * 1000\n",
    "    df_hybrid['GRPMR'] = df_hybrid['grle'] * 1000\n",
    "\n",
    "    df_hybrid = parse_date(os.path.basename(grib_in), df_hybrid, 'valid_time')\n",
    "\n",
    "    \n",
    "    # FINAL FORMATTING\n",
    "    vertical_layers_df = df_hybrid.copy()\n",
    "    # rename columns\n",
    "    vertical_layers_df.rename(columns={'gh':'Z', 't':'TEMPK', 'w': 'W', 'q': 'VAPMR', 'x':'IX', 'y':'JX', 'hybrid':'LEVEL'}, inplace=True)\n",
    "\n",
    "    # change field types\n",
    "    vertical_layers_df = change_field_type(vertical_layers_df, ['LEVEL', 'MYR', 'MMO', 'MDAY', 'MHR', 'IX', 'JX', \\\n",
    "                                                                'PRES', 'Z', 'WD', 'RH'], 'int')\n",
    "    # export ascii\n",
    "    vertical_layers_ascii = vertical_layers_df[vertical_vars]\n",
    "\n",
    "    # export csv (for QC)\n",
    "    vertical_vars_csv = vertical_vars + ['IX', 'JX', 'VAPMR', 'pres', 'TEMPK', 'u', 'v', 'latitude', 'longitude']\n",
    "    vertical_layers_csv = vertical_layers_df.loc[:, vertical_vars_csv]\n",
    "\n",
    "    # * = used for calculations\n",
    "    vertical_layers_csv.rename(columns={'pres':'p*', 'VAPMR':'qq*', 'TEMPK': 'tk*', 'u': 'u*', \\\n",
    "                                       'v':'v*', 'valid_time':'valid_time*'}, inplace=True)\n",
    "\n",
    "    # EXPORT CSV\n",
    "    # write log file (CSV)\n",
    "    vertical_layers_csv.to_csv(log_out, index=False)\n",
    "\n",
    "\n",
    "    # FORTRAN FORMATTING\n",
    "    # Fortran formatting for each column\n",
    "    vertical_layers_ascii = vertical_layers_ascii.round({'PRES': 1})\n",
    "    vertical_layers_ascii = vertical_layers_ascii.round({'RAIN': 2})\n",
    "    vertical_layers_ascii = vertical_layers_ascii.round({'RADSW': 1})\n",
    "    vertical_layers_ascii = vertical_layers_ascii.round({'RADLW': 1})\n",
    "    vertical_layers_ascii = vertical_layers_ascii.round({'T2': 1})\n",
    "    vertical_layers_ascii = vertical_layers_ascii.round({'Q2': 2})\n",
    "    vertical_layers_ascii = vertical_layers_ascii.round({'WD10': 1})\n",
    "    vertical_layers_ascii = vertical_layers_ascii.round({'WS10': 1})\n",
    "    vertical_layers_ascii = vertical_layers_ascii.round({'SST': 1})\n",
    "\n",
    "    # apply formatting to each pandas dataframe column\n",
    "    # https://stackoverflow.com/questions/30904333/write-pandas-dataframe-to-file-using-fortran-format-string\n",
    "    format_string = '(i4, i6, f6.1, i4, f5.1, f6.2, i3, f5.2, f6.3, f6.3, f6.3, f6.3, f6.3)'\n",
    "    header_line = ff.FortranRecordWriter(format_string)\n",
    "    vertical_layers_ascii = vertical_layers_ascii.apply(lambda x : header_line.write(x.values),axis=1)\n",
    "    \n",
    "    return vertical_layers_ascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9a94ebe-8535-41a3-8204-70f6d4a0e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SURFACE / TOP\n",
    "# in = path of grib file, out = Fortran-formatted dataframe\n",
    "def process_surface(grib_in, log_out):\n",
    "    \n",
    "    # SURFACE\n",
    "    ds = xr.open_dataset(grib_in, engine='cfgrib', \\\n",
    "                         backend_kwargs={'filter_by_keys': {'stepType': 'instant', 'typeOfLevel': 'surface'}})\n",
    "    # for v in ds:\n",
    "    #     print(\"{}, {}, {}\".format(v, ds[v].attrs['long_name'], ds[v].attrs['units']))\n",
    "\n",
    "    df_surface = create_df(ds, ['sp', 'orog', 'prate', 'snowc', 'dswrf'])\n",
    "    \n",
    "    df_surface = df_surface.reset_index()  # so x, y show up\n",
    "\n",
    "    # add 10 to x (i) and y (j)\n",
    "    df_surface['x'] = df_surface['x'] + 10\n",
    "    df_surface['y'] = df_surface['y'] + 10\n",
    "\n",
    "    # do HRRR surface conversions\n",
    "    #df_surface['PRES'] = calc_slpressure(df_surface, 'sp', 'orog', 't')\n",
    "    df_surface['RAIN'] = calc_rainfall(df_surface, 'prate')\n",
    "    df_surface['SC'] = calc_snowcover(df_surface, 'snowc')\n",
    "    \n",
    "    df_surface = parse_date(os.path.basename(grib_in), df_surface, 'valid_time')\n",
    "    \n",
    "    \n",
    "    # HEIGHT ABOVE GROUND - 2 METERS\n",
    "    ds = xr.open_dataset(grib_in, engine='cfgrib', \\\n",
    "                         backend_kwargs={'filter_by_keys': {'level': 2, 'typeOfLevel': 'heightAboveGround'}})\n",
    "\n",
    "    df_hag2 = create_df(ds, ['t2m', 'sh2', 'r2'])\n",
    "    \n",
    "    df_hag2 = df_hag2.reset_index()\n",
    "\n",
    "    # add 10 to x (i) and y (j)\n",
    "    df_hag2['x'] = df_hag2['x'] + 10\n",
    "    df_hag2['y'] = df_hag2['y'] + 10\n",
    "\n",
    "    # do HRRR 2 m conversions\n",
    "    # T2 = t2m (K, no conversion needed)\n",
    "    # Q2 = sh2 * 1000 (g/kg, no conversion needed)\n",
    "    df_hag2['sh2'] = df_hag2['sh2'] * 1000\n",
    "    # R2?\n",
    "    # https://carnotcycle.wordpress.com/2012/08/04/how-to-convert-relative-humidity-to-absolute-humidity/\n",
    "    # df_hag2['R2'] = (6.112 * np.exp((17.67 * (df_hag2['t2m']-273.15))/(df_hag2['t2m']-29.65)) * df_hag2['r2'] * 2.1674) / df_hag2['t2m']\n",
    "\n",
    "    df_hag2 = parse_date(os.path.basename(grib_in), df_hag2, 'valid_time')\n",
    "    \n",
    "    \n",
    "    # HEIGHT ABOVE GROUND - 10 METERS\n",
    "    ds = xr.open_dataset(grib_in, engine='cfgrib', \\\n",
    "                         backend_kwargs={'filter_by_keys': {'level': 10, 'typeOfLevel': 'heightAboveGround'}})\n",
    "\n",
    "    df_hag10 = create_df(ds, ['u10', 'v10', 'si10'])\n",
    "    \n",
    "    df_hag10 = df_hag10.reset_index()\n",
    "\n",
    "    # add 10 to x (i) and y (j)\n",
    "    df_hag10['x'] = df_hag10['x'] + 10\n",
    "    df_hag10['y'] = df_hag10['y'] + 10\n",
    "\n",
    "    # do HRRR 10 m conversions\n",
    "    df_hag10['WD10'] = calc_winddir(df_hag10, 'longitude', 'u10', 'v10')\n",
    "    df_hag10['WS10'] = calc_windspeed(df_hag10, 'u10', 'v10')\n",
    "    df_hag10['SST'] = 0.0\n",
    "\n",
    "    df_hag10 = parse_date(os.path.basename(grib_in), df_hag10, 'valid_time')\n",
    "    \n",
    "    \n",
    "    # NOMINAL TOP\n",
    "    ds = xr.open_dataset(grib_in, engine='cfgrib', backend_kwargs={'filter_by_keys': {'typeOfLevel': 'nominalTop'}})\n",
    "\n",
    "    df_top = create_df(ds, ['ulwrf'])\n",
    "\n",
    "    df_top = df_top.reset_index()\n",
    "\n",
    "    # add 10 to x (i) and y (j)\n",
    "    df_top['x'] = df_top['x'] + 10\n",
    "    df_top['y'] = df_top['y'] + 10\n",
    "\n",
    "    df_top = parse_date(os.path.basename(grib_in), df_top, 'valid_time')\n",
    "\n",
    "    \n",
    "    # JOIN THE SURFACE / TOP LAYERS\n",
    "    # remove duplicate columns (between data frames) - do this before merge\n",
    "    df_hag2 = remove_dup_cols(df_hag2, df_surface, ['y', 'x'])\n",
    "    df_hag10 = remove_dup_cols(df_hag10, df_surface, ['y', 'x'])\n",
    "    df_top = remove_dup_cols(df_top, df_surface, ['y', 'x'])\n",
    "\n",
    "    # do a triple merge (to join all surface / top layers on (IX, JX))\n",
    "    surface_layers_df = pd.merge(df_surface, pd.merge(df_top, pd.merge(df_hag2, df_hag10, on=['y', 'x']), on=['y', 'x']), on=['y', 'x'])\n",
    "    \n",
    "    \n",
    "    # FINAL CALCULATION\n",
    "    # do HRRR surface conversions\n",
    "    surface_layers_df['PRES'] = calc_slpressure(surface_layers_df, 'sp', 'orog', 't2m')\n",
    "    \n",
    "    \n",
    "    # FINAL FORMATTING\n",
    "    # drop duplicate fields (from merge)\n",
    "    surface_layers_df.drop([i for i in list(surface_layers_df.columns) if '_x' in i or '_y' in i], axis=1, inplace=True)\n",
    "    # rename columns\n",
    "    surface_layers_df.rename(columns={'dswrf':'RADSW', 'ulwrf':'RADLW', 't2m': 'T2', 'sh2': 'Q2', 'r2':'R2', 'x':'IX', 'y':'JX'}, inplace=True)\n",
    "    \n",
    "    \n",
    "    surface_layers_df['SC'] = surface_layers_df['SC'].round()\n",
    "    # change field types\n",
    "    surface_layers_df = change_field_type(surface_layers_df, ['MYR', 'MMO', 'MDAY', 'MHR', 'IX', 'JX', 'SC'], 'int')\n",
    "\n",
    "    # export ascii\n",
    "    surface_layers_ascii = surface_layers_df[surface_vars]\n",
    "\n",
    "    # export csv (for QC)\n",
    "    surface_vars_csv = surface_vars + ['sp', 'orog', 'u10', 'v10', 'prate', 'snowc', 'latitude', 'longitude', 'valid_time']\n",
    "    surface_layers_csv = surface_layers_df.loc[:, surface_vars_csv]\n",
    "\n",
    "    # * = used for calculations\n",
    "    surface_layers_csv.rename(columns={'sp':'P*', 'orog':'h*', 'T2': 'T2*', 'u10':'u10*', 'v10':'v10*', 'prate': 'prate_RAIN*', \\\n",
    "                                       'snowc':'snowc_SC*', 'valid_time':'valid_time*'}, inplace=True)\n",
    "    \n",
    "    # write log file (CSV)\n",
    "    surface_layers_csv.to_csv(log_out, index=False)\n",
    "    \n",
    "    \n",
    "    # FORTRAN FORMATTING\n",
    "    # Fortran formatting for each column\n",
    "    surface_layers_ascii = surface_layers_ascii.round({'PRES': 1})\n",
    "    surface_layers_ascii = surface_layers_ascii.round({'RAIN': 2})\n",
    "    surface_layers_ascii = surface_layers_ascii.round({'RADSW': 1})\n",
    "    surface_layers_ascii = surface_layers_ascii.round({'RADLW': 1})\n",
    "    surface_layers_ascii = surface_layers_ascii.round({'T2': 1})\n",
    "    surface_layers_ascii = surface_layers_ascii.round({'Q2': 2})\n",
    "    surface_layers_ascii = surface_layers_ascii.round({'WD10': 1})\n",
    "    surface_layers_ascii = surface_layers_ascii.round({'WS10': 1})\n",
    "    surface_layers_ascii = surface_layers_ascii.round({'SST': 1})\n",
    "\n",
    "    # apply formatting to each pandas dataframe column\n",
    "    # https://stackoverflow.com/questions/30904333/write-pandas-dataframe-to-file-using-fortran-format-string\n",
    "    format_string = '(i4, i2, i2, i2, i3, i3, f7.1, f5.2, i2, f8.1, f8.1, f8.1, f8.2, f8.1, f8.1, f8.1)'\n",
    "    header_line = ff.FortranRecordWriter(format_string)\n",
    "    surface_layers_ascii = surface_layers_ascii.apply(lambda x : header_line.write(x.values),axis=1)\n",
    "    \n",
    "    return surface_layers_ascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8d968db-2e82-499e-8300-a33563edd5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export final combined ASCII file!\n",
    "# in = output file path, input vertical layers, input surface layers\n",
    "# out = final combined ascii file (in Fortran format)\n",
    "def export_ascii(output_file, in_vertical, in_surface):\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        count = 0\n",
    "        for row in in_surface:\n",
    "            outfile.write(row+'\\n')\n",
    "\n",
    "            # there are 3025 x, y pairs, and 50 levels for each x, y pair - loop through them using a range\n",
    "            for i in range(count+0, 3025*50, 3025):\n",
    "                row = in_vertical[i]\n",
    "                outfile.write(row+'\\n')\n",
    "            count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be289a86-47f2-4db3-8255-516238f29926",
   "metadata": {},
   "source": [
    "# LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a34d7b-9ded-434e-8a83-3d79b809d252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t00z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t01z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t02z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t03z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t04z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t05z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t06z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t07z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t08z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t09z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t10z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t11z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t12z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t13z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t14z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t15z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t16z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t17z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t18z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t19z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t20z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t21z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t22z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.03.2023.noaa.sg\\hrrr.t23z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t00z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t01z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t02z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t03z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t04z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t05z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t06z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t07z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t08z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t09z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t10z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t11z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t12z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t13z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t14z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t15z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t16z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t17z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t18z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t19z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t20z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t21z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t22z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.04.2023.noaa.sg\\hrrr.t23z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t00z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t01z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t02z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t03z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t04z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t05z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t06z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t07z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t08z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t09z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t10z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t11z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t12z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t13z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t14z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t15z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t16z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t17z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t18z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t19z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t20z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t21z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t22z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.05.2023.noaa.sg\\hrrr.t23z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t00z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t01z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t02z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t03z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t04z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t05z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t06z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t07z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t08z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t09z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t10z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t11z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t12z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t13z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t14z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t15z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t16z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t17z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t18z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t19z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t20z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t21z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t22z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.06.2023.noaa.sg\\hrrr.t23z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t00z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t01z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t02z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t03z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t04z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t05z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t06z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t07z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t08z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t09z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t10z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t11z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t12z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t13z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t14z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t15z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t16z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t17z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t18z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t19z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t20z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t21z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t22z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.07.2023.noaa.sg\\hrrr.t23z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.08.2023.noaa.sg\\hrrr.t00z.wrfnatf00.sg.dat\n",
      "P:\\CMaderia\\test\\02.08.2023.noaa.sg\\hrrr.t01z.wrfnatf00.sg.dat\n"
     ]
    }
   ],
   "source": [
    "for infolder in infolder_list:\n",
    "    grib_list = glob(os.path.join(infolder, '*.grib2'))\n",
    "    for g in grib_list:\n",
    "        \n",
    "        # DIRECTORY SET UP\n",
    "        out_asciifolderpath = os.path.join(out_asciifolder, os.path.basename(infolder))\n",
    "        # make the log folder directory to store the output for each individual grib run, if it does not already exist\n",
    "        if os.path.exists(out_asciifolderpath) == False: os.mkdir(out_asciifolderpath)\n",
    "        \n",
    "        out_logfolderpath = os.path.join(out_asciifolderpath, 'log')\n",
    "        # make the log folder directory to store the output for each individual grib run, if it does not already exist\n",
    "        if os.path.exists(out_logfolderpath) == False: os.mkdir(out_logfolderpath)\n",
    "        \n",
    "        # FILE EXPORT\n",
    "        # process vertical layers first, format them, and export log\n",
    "        out_verticallog = os.path.join(out_logfolderpath, os.path.basename(g).replace('.grib2', '') + '_vertical_data' + '.csv')\n",
    "        #print(out_verticallog)\n",
    "        vertical = process_hybrid(g, out_verticallog)\n",
    "        \n",
    "        # process surface layers next, format them, and export log\n",
    "        out_surfacelog = os.path.join(out_logfolderpath, os.path.basename(g).replace('.grib2', '') + '_surface_data' + '.csv')\n",
    "        #print(out_surfacelog)\n",
    "        surface = process_surface(g, out_surfacelog)\n",
    "        \n",
    "        # combine vertical and surface, and export Fortran-formatted ascii\n",
    "        out_ascii_combined = os.path.join(out_asciifolderpath, os.path.basename(g)).replace('.grib2', '.dat')\n",
    "        print(out_ascii_combined)\n",
    "        export_ascii(out_ascii_combined, vertical, surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3571bfb9-8a92-40dd-bcb3-a45d1c0624fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db8045-407d-4f74-9424-51ae59cac817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f430ffb-8cbd-4c96-bf1e-ce5fda30458d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "django",
   "language": "python",
   "name": "django"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
